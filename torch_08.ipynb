{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 処理パタン\n",
    "\n",
    "結局のところ、処理パタンは決まっていて、それを呼び出すだけ。\n",
    "\n",
    "詳細の理解は、別途、学習した方が良い。コードと理論の対応関係を大まかに理解させるために、\n",
    "６００＋４００ページもの分厚い本が存在するのだと理解した。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "    print(f'device count: {torch.cuda.device_count()}')\n",
    "    print(f'device name: {torch.cuda.get_device_name()}')\n",
    "    print(f'device capability: {torch.cuda.get_device_capability()}')\n",
    "\n",
    "    return device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### １階テンソル化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform1 = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0.5, 0.5),\n",
    "    transforms.Lambda(lambda x: x.view(-1)),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正規化のみ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform2 = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0.5, 0.5),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データローダ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練データと予測データの分割\n",
    "\n",
    "キーワード引数stratifyは、２値分類で、訓練データと予測データの中にある、正例と負例の比率が同じになることを保証する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, train_size=0.7, test_size=0.3, random_state=123, stratify=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ミニバッチ処理\n",
    "\n",
    "ミニバッチ処理の最小単位のバッチのサイズをbatch_sizeで指定している。一般的に、この値が小さい程、並列性は低くなるが、精度は上がることが知られている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 500\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    '''\n",
    "    隠れ層が１層のNN\n",
    "    '''\n",
    "    def __init__(self, n_input, n_hidden, n_output):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(n_input, n_hidden)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.l2 = nn.Linear(n_hidden, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.l2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, n_input, n_hidden, n_output): # n_input == 32 * 14 * 14 = 6272. See p.344--p.347.\n",
    "        super().__init__()\n",
    "        # CIFAR-10データセット用の次元\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3)\n",
    "        self.conv2 = nn.Conv2d(32, 32, 3)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d((2, 2))\n",
    "        # １次元化\n",
    "        self.flatten = nn.Flatten()\n",
    "        # 上記Netと同様\n",
    "        self.l1 = nn.Linear(n_input, n_hidden)\n",
    "        self.l2 = nn.Linear(n_hidden, n_output)\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            self.conv1,\n",
    "            self.relu,\n",
    "            self.conv2,\n",
    "            self.relu,\n",
    "            self.maxpool,\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            self.l1,\n",
    "            self.relu,\n",
    "            self.l2,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.features(x)\n",
    "        x2 = self.flatten(x1)\n",
    "        x3 = self.classifier(x2)\n",
    "\n",
    "        return x3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_parameters(net):\n",
    "    for parameter in net.named_parameters():\n",
    "        print(parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 損失計算（動作確認用）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_computational_graph_for_loss(loader, device, net, criterion):\n",
    "    # データローダから最初の１セット取得（計算グラフ作成のため）\n",
    "    for images, labels in loader:\n",
    "        break\n",
    "    # デバイス（GPU/CPU割当）\n",
    "    inputs = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    # 予測計算\n",
    "    outputs = net(inputs)\n",
    "    # 損失計算\n",
    "    loss = criterion(outputs, labels)\n",
    "    # 計算グラフの表示\n",
    "    make_dot(loss, params=dict(net.named_parameters()))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習・追加学習用関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = np.zeros((0, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "＊ `net.train()`は「訓練中」、`net.eval()`は「予測中」の別をPyTorchに知らせる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(net, optimizer, criterion, num_epochs, train_loader, test_loader, device, history):\n",
    "    base_epochs = len(history)\n",
    "\n",
    "    for epoch in range(base_epochs, num_epochs + base_epochs):\n",
    "        # 1エポックあたりの正解数（精度計算用）\n",
    "        n_train_acc, n_val_acc = 0, 0\n",
    "        # 1エポックあたりの累積損失（平均化前）\n",
    "        train_loss, val_loss = 0, 0\n",
    "        # 1エポックあたりのデータ数\n",
    "        n_train, n_test = 0, 0\n",
    "\n",
    "        ##### 訓練 #####\n",
    "        net.train() # ドロップアウト関数nn.DropoutやBN関数nn.BatchNorm2dに訓練中であることを伝える関数\n",
    "\n",
    "        for inputs, labels in tqdm(train_loader):\n",
    "            train_batch_size = len(labels)\n",
    "            n_train += train_batch_size\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            predicted = torch.max(outputs, 1)[1]\n",
    "            train_acc += (predicted == labels).sum().item()\n",
    "            train_loss += loss.item() * train_batch_size\n",
    "\n",
    "        train_acc /= n_train\n",
    "        train_loss /= n_train\n",
    "\n",
    "        ##### 予測 #####\n",
    "        net.eval() # ドロップアウト関数nn.DropoutやBN関数nn.BatchNorm2dに予測中であることを伝える関数\n",
    "\n",
    "        for inputs_test, labels_test in test_loader:\n",
    "            test_batch_size = len(labels_test)\n",
    "            n_test += test_batch_size\n",
    "\n",
    "            inputs_test = inputs_test.to(device)\n",
    "            labels_test = labels_test.to(device)\n",
    "\n",
    "            outputs_test = net(inputs_test)\n",
    "            loss_test = criterion(outputs_test, labels_test)\n",
    "\n",
    "            predict_test = torch.max(outputs_test, 1)[1]\n",
    "            val_acc += (predict_test == labels_test).sum().item()\n",
    "            val_loss += loss_test.item() * test_batch_size\n",
    "\n",
    "        val_acc /= n_test\n",
    "        val_loss /= n_test\n",
    "\n",
    "        item = np.array([epoch, train_loss, train_acc, val_loss, val_acc])\n",
    "        history = np.vstack((history, item))\n",
    "        if epoch % (num_epochs / 100) == 0:\n",
    "            print (f'Epoch [{epoch}/{num_epochs}], loss: {train_loss:.5f} acc: {train_acc:.5f} val_loss: {val_loss:.5f}, val_acc: {val_acc:.5f}')\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習ログ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習前後でのlossとaccuracyを表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_history_head_and_tail(history):\n",
    "    print(f'Initial state: loss = {history[0, 3]:.5f} / accuracy = {history[0, 4]:.5f}')\n",
    "    print(f'Final state: loss = {history[-1, 3]:.5f} / accuracy = {history[-1, 4]:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lossをepochの関数としてプロット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_history(history):\n",
    "    plt.plot(history[:, 0], history[:, 1], label='train loss')\n",
    "    plt.plot(history[:, 0], history[:, 3], label='val loss')\n",
    "    plt.legend()\n",
    "    plt.title('Learning Curve wrt Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accuracyをepochの関数としてプロット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy_history(history):\n",
    "    plt.plot(history[:, 0], history[:, 2], label='train acc')\n",
    "    plt.plot(history[:, 0], history[:, 4], label='val acc')\n",
    "    plt.legend()\n",
    "    plt.title('Learning Curve wrt Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 予測結果の可視化\n",
    "\n",
    "いちばん重要なところでありながら、本では幾つかが省略されている。著者のGitHubを見るも、全てを公開はしている訳ではない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 回帰の場合\n",
    "\n",
    "一般的に、特徴量が２つ以上になるので、普通にはプロットできない。\n",
    "\n",
    "ここでは、特徴量が２つの場合のみの予測結果の可視化関数を定義する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_regression(data, net):\n",
    "    x = data[:, 0]\n",
    "    y = data[:, 1]\n",
    "\n",
    "    xse = np.array((x.min(), x.max())).reshape(-1, 1)\n",
    "    Xse = torch.tensor(xse).float()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 端っこの点を取って、それを結べば良いだけ（直線の場合はこれで良い）\n",
    "        Yse = net(Xse)\n",
    "    \n",
    "    plt.scatter(x, y, s=10, c='b')\n",
    "    plt.plot(Xse.data, Yse.data, c='k')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分類の場合\n",
    "\n",
    "- [sklearnの場合](https://www.dskomei.com/entry/2018/03/04/125249)\n",
    "    - [著者による解説](https://qiita.com/makaishi2/items/2d0dd015ac1cbd86dd3f)\n",
    "\n",
    "torchの場合も基本は同じ。`numpy.ravel`と`np.meshgrid`を使うとできる。[こちら](https://zenn.dev/curry/articles/b8ca6a3b668f3a)をまとめる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 画像データ分類の場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images_labels(loader, classes, net, device):\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    for images, labels in loader:\n",
    "        num_show = min(20, len(labels))\n",
    "        predicted = None\n",
    "        if net is not None:\n",
    "            net = net.to(device)\n",
    "            predicted = net(images.to(device))\n",
    "        for ii in range(num_show):\n",
    "            ax = plt.subplot(2, 10, ii + 1)\n",
    "\n",
    "            image = images[ii]\n",
    "            label_name = classes[labels[ii]]\n",
    "\n",
    "            plt.imshow(image)\n",
    "\n",
    "            title_name = label_name\n",
    "            color = 'k'\n",
    "\n",
    "            if net is not None:\n",
    "                predicted_name = classes[predicted[ii]]\n",
    "                if label_name != predicted_name:\n",
    "                    color = 'r'\n",
    "                title_name = f'[{label_name}] {predicted_name}'\n",
    "\n",
    "            ax.set_title(title_name, fontsize=20, c=color)\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "        break\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 乱数を固定する（結果の再現性を担保するため）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_seed(seed=123):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True # GPUで結果がdeterministicになるように\n",
    "    torch.use_deterministic_algorithms = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルをシリアライズする\n",
    "\n",
    "実用上、非常に重要なことなのに、この点についても振れられていないので、自分で調査した。\n",
    "\n",
    "様々なパッケージがあり、どうやら確立された方法がないらしい。\n",
    "\n",
    "- [cloudpickle](https://github.com/cloudpipe/cloudpickle): sagemakerも内部的に使っている（依存関係がある）\n",
    "- [joblib](https://joblib.readthedocs.io/en/stable/): pickleより遅いが、ファイルサイズは小さくなる利点がある\n",
    "- [h5py](https://docs.h5py.org/en/stable/): 科学技術計算での標準的フォーマットHDF5\n",
    "- [dill](https://pypi.org/project/dill/): joblibと同様、pickleよりファイルサイズが小さくなるらしい\n",
    "\n",
    "Python標準の`pickle`では、`torch.tensor`がもつ、内部状態を保存する際、\n",
    "問題が発生するらしい。最終的に、独自関数が用意されていることが分かった。\n",
    "\n",
    "- [torch.save/load](https://wandb.ai/wandb_fc/japanese/reports/PyTorch---VmlldzoxNTAyODQy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習済みモデル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 保存\n",
    "\n",
    "state_dictを使うことが推奨されている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'save/to/path/model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 読込\n",
    "\n",
    "state_dictを使う場合、以下のように、モデルの定義が必要になる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(n_input, n_hidden, n_output)\n",
    "model.load_state_dict(torch.load('load/from/path/model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習途中で中断（チェックポイント）\n",
    "\n",
    "#### 保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': net.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            }, 'save/to/path/model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 読込"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(n_input, n_hidden, n_output)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9) ## just example\n",
    "\n",
    "checkpoint = torch.load('load/from/path/model.pth')\n",
    "net.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使い方の例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 変数の初期化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input, n_hidden, n_output = 784, 100, 10\n",
    "\n",
    "device = get_device()\n",
    "net = Net(n_input, n_hidden, n_output).to(device)\n",
    "\n",
    "lr = 0.01\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 100\n",
    "history = np.zeros((0,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデル構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 乱数初期化\n",
    "torch_seed()\n",
    "\n",
    "# 学習\n",
    "history = fit(net, optimizer, criterion, num_epochs, train_loader, test_loader, device, history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデル確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_model_parameters(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_computational_graph_for_loss(train_loader, device, net, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 収束性の確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_history_head_and_tail(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 予測結果の可視化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
